{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":["17U3J4Of9SMq"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU","widgets":{"application/vnd.jupyter.widget-state+json":{"68f6512679b145929481c8411bac96db":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5865bd39e32f43e78d78bd28bad09338","IPY_MODEL_f52e232eeb2b437795fe80f3a09d51f4","IPY_MODEL_63c5e7ce45a949088da12e6b2b9716ca"],"layout":"IPY_MODEL_c36be66b06fa4c13b21cdffe4e93c8dd"}},"5865bd39e32f43e78d78bd28bad09338":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d057b61f296462c836948f7c10fbb34","placeholder":"​","style":"IPY_MODEL_ce892377d2e94886bc375962456f453a","value":"Downloading: 100%"}},"f52e232eeb2b437795fe80f3a09d51f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_733aa71e43294c2291f198a3718a1e2f","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7cae4e3609f4ff6bc30d8d82c062beb","value":231508}},"63c5e7ce45a949088da12e6b2b9716ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_281ef110af3a478ab10cc674e438dba5","placeholder":"​","style":"IPY_MODEL_f0ffe803085c4f6cad3bac7031f20d75","value":" 226k/226k [00:00&lt;00:00, 1.08MB/s]"}},"c36be66b06fa4c13b21cdffe4e93c8dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d057b61f296462c836948f7c10fbb34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce892377d2e94886bc375962456f453a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"733aa71e43294c2291f198a3718a1e2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7cae4e3609f4ff6bc30d8d82c062beb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"281ef110af3a478ab10cc674e438dba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0ffe803085c4f6cad3bac7031f20d75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00946a24e060431698f283631b818811":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1bf0a05be6ae4af0b1f00984ad05a67a","IPY_MODEL_e6791faa2931456ba8168c6a71493eff","IPY_MODEL_8ebcdae279f14b3da48e95dd0dd47ff4"],"layout":"IPY_MODEL_f54129ae4bee4475aa7ac9b07b48aed1"}},"1bf0a05be6ae4af0b1f00984ad05a67a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_052f0d52a79a45b884cf8d2ad1fb6fb3","placeholder":"​","style":"IPY_MODEL_84dae75de8554624b8125d5d8e5bc506","value":"Downloading: 100%"}},"e6791faa2931456ba8168c6a71493eff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_954ed14a16d841dfb5bcebceabdd8ccc","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c8cd8aff7afc471ab8f83c09712b9ac9","value":28}},"8ebcdae279f14b3da48e95dd0dd47ff4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecbce6c5558a45f28dd2eb88faf810a2","placeholder":"​","style":"IPY_MODEL_74c427295f3e475a8980a4ebad8027e1","value":" 28.0/28.0 [00:00&lt;00:00, 346B/s]"}},"f54129ae4bee4475aa7ac9b07b48aed1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"052f0d52a79a45b884cf8d2ad1fb6fb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84dae75de8554624b8125d5d8e5bc506":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"954ed14a16d841dfb5bcebceabdd8ccc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8cd8aff7afc471ab8f83c09712b9ac9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecbce6c5558a45f28dd2eb88faf810a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c427295f3e475a8980a4ebad8027e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"050f969d945f41f1b71a5d6ec4ccd568":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae9c8a067493416ca46cbd6ef32ab003","IPY_MODEL_b99a8ba3d40a4c5aa5621d834739e9a4","IPY_MODEL_26132a0445bd44abbf3405c45f7e14e6"],"layout":"IPY_MODEL_dc374b94d938489d9a16463ea34ea57e"}},"ae9c8a067493416ca46cbd6ef32ab003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de016b5885b64d9ab062439498a21217","placeholder":"​","style":"IPY_MODEL_9dccd00cdfde4f918fafc4c99ea0b318","value":"Downloading: 100%"}},"b99a8ba3d40a4c5aa5621d834739e9a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0850da5e960a4fd3bfa08c873ec35691","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3de4f63bcbc04c118798458ea10cfe52","value":570}},"26132a0445bd44abbf3405c45f7e14e6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_079c8999a121496b8152e875cfdc0261","placeholder":"​","style":"IPY_MODEL_d8309b7c01ab4910a6bb433a8284ef96","value":" 570/570 [00:00&lt;00:00, 13.9kB/s]"}},"dc374b94d938489d9a16463ea34ea57e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de016b5885b64d9ab062439498a21217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dccd00cdfde4f918fafc4c99ea0b318":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0850da5e960a4fd3bfa08c873ec35691":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3de4f63bcbc04c118798458ea10cfe52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"079c8999a121496b8152e875cfdc0261":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8309b7c01ab4910a6bb433a8284ef96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"523c3e24832c442b9424832503d2cbf2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66425f28dd474ac4a51caf270c6547b3","IPY_MODEL_a66d04e7316d413ebf68e7377130145c","IPY_MODEL_bb0cc5a6fe754e4c87a4210c6e24c71c"],"layout":"IPY_MODEL_d0267f2000b94bd4aa887aee57bda5a9"}},"66425f28dd474ac4a51caf270c6547b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8921a6559c60495a913f39d7e2fb2015","placeholder":"​","style":"IPY_MODEL_b4563b3104a940d19da9d96b3a4454fc","value":"Downloading: 100%"}},"a66d04e7316d413ebf68e7377130145c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d683c4a261f4bae83b44a9acd6496cb","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66de9b01a867447ba968426805e34284","value":231508}},"bb0cc5a6fe754e4c87a4210c6e24c71c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d16b556522804d409ae11f12f810f979","placeholder":"​","style":"IPY_MODEL_f3141fb7a1c1479b8c8e6c91bf21833a","value":" 226k/226k [00:00&lt;00:00, 8.80kB/s]"}},"d0267f2000b94bd4aa887aee57bda5a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8921a6559c60495a913f39d7e2fb2015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4563b3104a940d19da9d96b3a4454fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d683c4a261f4bae83b44a9acd6496cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66de9b01a867447ba968426805e34284":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d16b556522804d409ae11f12f810f979":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3141fb7a1c1479b8c8e6c91bf21833a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b51eabc2bf7447beb824f7657664ec76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d0815741116406e8ab2a6b0dbcb632d","IPY_MODEL_3ac7b15b9e6b41dc9d2fb522617dedca","IPY_MODEL_f0145c0b5e29456fbb7c7cb2e2844edd"],"layout":"IPY_MODEL_cc038c69cd614cdbb388ff11828014e5"}},"6d0815741116406e8ab2a6b0dbcb632d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a1e744e83334dc6bcb6d830f4c67dd8","placeholder":"​","style":"IPY_MODEL_3ad27e80bf69430b824b77ece0f5a089","value":"Downloading: 100%"}},"3ac7b15b9e6b41dc9d2fb522617dedca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f33be4f84ee241daa0d1ec21430dd50b","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82ef32302e054c89891034546156b4b8","value":28}},"f0145c0b5e29456fbb7c7cb2e2844edd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c02930a5bcc4b8998b2248e3ffaabb1","placeholder":"​","style":"IPY_MODEL_3f89a46ece1d40c6a77c1fa10bac26a6","value":" 28.0/28.0 [00:00&lt;00:00, 245B/s]"}},"cc038c69cd614cdbb388ff11828014e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a1e744e83334dc6bcb6d830f4c67dd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ad27e80bf69430b824b77ece0f5a089":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f33be4f84ee241daa0d1ec21430dd50b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82ef32302e054c89891034546156b4b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c02930a5bcc4b8998b2248e3ffaabb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f89a46ece1d40c6a77c1fa10bac26a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d63d72b83654d5bae2d65d34bfbe969":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3a35df7721844068cf95305dd411563","IPY_MODEL_4e98e7d1cdee4e3499269c4048f0389a","IPY_MODEL_0aa33ed6bb3a4d06808337eca5ce449d"],"layout":"IPY_MODEL_a0aa51c527b64cd79cec70b3755f0fd0"}},"a3a35df7721844068cf95305dd411563":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bf4db606a014a4c9738b8ca7d288439","placeholder":"​","style":"IPY_MODEL_29a4a95ede4a44169390266920eb40f6","value":"Downloading: 100%"}},"4e98e7d1cdee4e3499269c4048f0389a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f0f21815aca4363b365003375db1d1e","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_38db8014334b4ccdab4b7a0630e5d53a","value":570}},"0aa33ed6bb3a4d06808337eca5ce449d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1623bb2f0d324bf38863785b4a4e2d17","placeholder":"​","style":"IPY_MODEL_38c56d7c5edb4449ae519e1e5b131cd5","value":" 570/570 [00:00&lt;00:00, 4.08kB/s]"}},"a0aa51c527b64cd79cec70b3755f0fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf4db606a014a4c9738b8ca7d288439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29a4a95ede4a44169390266920eb40f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f0f21815aca4363b365003375db1d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38db8014334b4ccdab4b7a0630e5d53a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1623bb2f0d324bf38863785b4a4e2d17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38c56d7c5edb4449ae519e1e5b131cd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9335d79924a4373b309e6c121a1dafe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4cbf8f5aa5a4466eb48dfaa4180f6e3b","IPY_MODEL_aabd1a98f1b240b392bd069d19d94141","IPY_MODEL_c7122f22e7f94cff81b0f7d0e83ef664"],"layout":"IPY_MODEL_cda5e57bb6864343802c3e1b0242c109"}},"4cbf8f5aa5a4466eb48dfaa4180f6e3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1983dc562c46466ba0bbdd74ac11e3ca","placeholder":"​","style":"IPY_MODEL_6abe69c752464604b84cc5d02e7012b6","value":"Downloading: 100%"}},"aabd1a98f1b240b392bd069d19d94141":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7acb0dd179bf444d8fce34ef2fefac28","max":536063208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e361dfd6f68f42ce8405f1cb5234b90c","value":536063208}},"c7122f22e7f94cff81b0f7d0e83ef664":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b981d51756a4539af8167abafe43f92","placeholder":"​","style":"IPY_MODEL_331a52ca5b4f4f56bf46855ba92beb9b","value":" 511M/511M [00:39&lt;00:00, 11.9MB/s]"}},"cda5e57bb6864343802c3e1b0242c109":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1983dc562c46466ba0bbdd74ac11e3ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6abe69c752464604b84cc5d02e7012b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7acb0dd179bf444d8fce34ef2fefac28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e361dfd6f68f42ce8405f1cb5234b90c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6b981d51756a4539af8167abafe43f92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"331a52ca5b4f4f56bf46855ba92beb9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNh8scXKx955","executionInfo":{"status":"ok","timestamp":1649508776628,"user_tz":-480,"elapsed":19251,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"aa3808ab-778d-4802-ecae-4217adb06a33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/\n"]}],"source":["import sys, os\n","import shutil\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","path_to_file = '/content/drive/My Drive/Colab Notebooks/'\n","print(path_to_file)\n","# change current path to the folder containing \"file_name\"\n","os.chdir(path_to_file)"]},{"cell_type":"code","source":["!pip install texthero==1.1.0 -q\n","!pip install demoji\n","!pip install emoji\n","import texthero as hero\n","import emoji\n","import demoji\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import nltk\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","import re\n","from texthero import stopwords\n","\n","pd.set_option('display.max_colwidth', 50)\n","\n","%matplotlib inline\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WaY4g4mDx-rC","executionInfo":{"status":"ok","timestamp":1649508833042,"user_tz":-480,"elapsed":23200,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"96f1b1d8-e2fe-4264-dc1a-e8c4dff4d9bb"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 235 kB 7.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.5 MB 64.9 MB/s \n","\u001b[K     |████████████████████████████████| 749 kB 48.8 MB/s \n","\u001b[?25hCollecting demoji\n","  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 665 kB/s \n","\u001b[?25hInstalling collected packages: demoji\n","Successfully installed demoji-1.1.0\n","Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 5.3 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=b613c1271ac47809ef1668317f3bf263ea3010dc2f80514707f3bd2167ff227e\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('./comments_and_tags.csv', sep = ',', index_col = 'Unnamed: 0')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"bXwrG1CXx-te","executionInfo":{"status":"ok","timestamp":1648885651502,"user_tz":-480,"elapsed":291,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"1aaad14c-4d79-4353-d19c-279066e4893e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id  sentiment search_term    tagger         author  \\\n","0  glngetr          0      ladvc3  SUPRATIK      nevabraun   \n","1  glndurd          1      ladvc3  SUPRATIK       Awake_4E   \n","2  glndfos          1      ladvc3  SUPRATIK  Cloud9forreal   \n","3  glnd58d          1      ladvc3  SUPRATIK       MacCoy69   \n","4  glnd2xt          1      ladvc3  SUPRATIK        Menuler   \n","\n","                                                body  created_utc    link_id  \\\n","0  Thanks but you’ve lost me at \\n\\n„If look at A...   1612214426  t3_ladvc3   \n","1  Awesome 😎! Why the moon 🤔 Let’s shoot AMC out ...   1612213448  t3_ladvc3   \n","2  If you look at AMCs business page you’ll find ...   1612213289  t3_ladvc3   \n","3  I bought 20 today, i am also a retarded dumb m...   1612213186  t3_ladvc3   \n","4  Ahh, my fellow Retard. I see the more and more...   1612213163  t3_ladvc3   \n","\n","                                           permalink  score       subreddit  \\\n","0  /r/wallstreetbets/comments/ladvc3/just_bought_...     21  wallstreetbets   \n","1  /r/wallstreetbets/comments/ladvc3/just_bought_...     25  wallstreetbets   \n","2  /r/wallstreetbets/comments/ladvc3/just_bought_...     58  wallstreetbets   \n","3  /r/wallstreetbets/comments/ladvc3/just_bought_...     13  wallstreetbets   \n","4  /r/wallstreetbets/comments/ladvc3/just_bought_...     11  wallstreetbets   \n","\n","                                           post_name              filename  \n","0  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv  \n","1  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv  \n","2  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv  \n","3  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv  \n","4  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv  "],"text/html":["\n","  <div id=\"df-6893a7f2-d8ae-46e3-9863-c0df04c08986\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>search_term</th>\n","      <th>tagger</th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>created_utc</th>\n","      <th>link_id</th>\n","      <th>permalink</th>\n","      <th>score</th>\n","      <th>subreddit</th>\n","      <th>post_name</th>\n","      <th>filename</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>glngetr</td>\n","      <td>0</td>\n","      <td>ladvc3</td>\n","      <td>SUPRATIK</td>\n","      <td>nevabraun</td>\n","      <td>Thanks but you’ve lost me at \\n\\n„If look at A...</td>\n","      <td>1612214426</td>\n","      <td>t3_ladvc3</td>\n","      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n","      <td>21</td>\n","      <td>wallstreetbets</td>\n","      <td>Just bought 860 of AMC, greetings from Germany...</td>\n","      <td>comments_batch_6.csv</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>glndurd</td>\n","      <td>1</td>\n","      <td>ladvc3</td>\n","      <td>SUPRATIK</td>\n","      <td>Awake_4E</td>\n","      <td>Awesome 😎! Why the moon 🤔 Let’s shoot AMC out ...</td>\n","      <td>1612213448</td>\n","      <td>t3_ladvc3</td>\n","      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n","      <td>25</td>\n","      <td>wallstreetbets</td>\n","      <td>Just bought 860 of AMC, greetings from Germany...</td>\n","      <td>comments_batch_6.csv</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>glndfos</td>\n","      <td>1</td>\n","      <td>ladvc3</td>\n","      <td>SUPRATIK</td>\n","      <td>Cloud9forreal</td>\n","      <td>If you look at AMCs business page you’ll find ...</td>\n","      <td>1612213289</td>\n","      <td>t3_ladvc3</td>\n","      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n","      <td>58</td>\n","      <td>wallstreetbets</td>\n","      <td>Just bought 860 of AMC, greetings from Germany...</td>\n","      <td>comments_batch_6.csv</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>glnd58d</td>\n","      <td>1</td>\n","      <td>ladvc3</td>\n","      <td>SUPRATIK</td>\n","      <td>MacCoy69</td>\n","      <td>I bought 20 today, i am also a retarded dumb m...</td>\n","      <td>1612213186</td>\n","      <td>t3_ladvc3</td>\n","      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n","      <td>13</td>\n","      <td>wallstreetbets</td>\n","      <td>Just bought 860 of AMC, greetings from Germany...</td>\n","      <td>comments_batch_6.csv</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>glnd2xt</td>\n","      <td>1</td>\n","      <td>ladvc3</td>\n","      <td>SUPRATIK</td>\n","      <td>Menuler</td>\n","      <td>Ahh, my fellow Retard. I see the more and more...</td>\n","      <td>1612213163</td>\n","      <td>t3_ladvc3</td>\n","      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n","      <td>11</td>\n","      <td>wallstreetbets</td>\n","      <td>Just bought 860 of AMC, greetings from Germany...</td>\n","      <td>comments_batch_6.csv</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6893a7f2-d8ae-46e3-9863-c0df04c08986')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6893a7f2-d8ae-46e3-9863-c0df04c08986 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6893a7f2-d8ae-46e3-9863-c0df04c08986');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["\n","\n","\n","\n","def do_preprocessing_stopwords(df_series):\n","    custom_pipeline = [hero.preprocessing.fillna, #remove NA\n","                          hero.preprocessing.lowercase, # lowercase\n","                          hero.preprocessing.remove_urls,\n","                          hero.preprocessing.remove_digits, # remove numbers\n","                          ] \n","    df_series = df_series.pipe(hero.clean, custom_pipeline)\n","\n","    df_series = df_series.apply(remove_emoji) # Remove emojis\n","\n","     # add custom stopwords\n","    default_stopwords = stopwords.DEFAULT\n","    new_words = ['already','also','comment','delete','even','literally','lolol','lololol','lolz','lols','lot','loll','lolololol','lolll','mean','na',\n","                'point','post','probably','put','reddit','remove','see','something','want','well']\n","    custom_stopwords = default_stopwords.union(set(new_words))\n","    df_series = hero.preprocessing.remove_stopwords(df_series, custom_stopwords) # remove stopwords\n","    lemmatize_text = df_series.apply(words_lemmatizer)\n","    \n","    df_series = hero.preprocessing.remove_whitespace(df_series) # remove redundant whitespace as the last step\n","    \n","    return df_series"],"metadata":{"id":"qEOQgKCrx-v0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U \"tensorflow-text==2.8.*\"\n","!pip install -q tf-models-official==2.7.0\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","import os\n","import pandas as pd\n","tf.get_logger().setLevel('ERROR')\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","\n","\n","from tensorflow.keras.models import Model\n","import numpy as np\n","from tensorflow.keras.layers import Input, Dense, Embedding, Activation, Flatten, Dropout\n","from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout, Concatenate, SimpleRNN,Bidirectional\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"metadata":{"id":"AhIx1_QQ9ZMP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649508932650,"user_tz":-480,"elapsed":28264,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"d5dde047-7b78-4c4c-e843-4df9974b4696"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 57.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 237 kB 58.7 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 7.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 48.6 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 58.3 MB/s \n","\u001b[K     |████████████████████████████████| 47.8 MB 1.2 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 6.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 70.0 MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.TPUStrategy(tpu)"],"metadata":{"id":"rGSL4DzA9ZOl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649508941139,"user_tz":-480,"elapsed":8495,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"31d1a68b-e07d-4ed8-d630-2fc734d4f228"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Running on TPU  ['10.73.82.42:8470']\n"]}]},{"cell_type":"code","source":["\n","!pip install SentencePiece\n","!pip install transformers\n","\n","from transformers import BertTokenizer,BertModel, BertConfig, TFBertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","from keras.models import load_model\n","\n","!pip install -q -U \"tensorflow-text==2.8.*\"\n","!pip install -q tf-models-official==2.7.0\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from official.nlp import optimization  # to create AdamW optimizer\n","import os\n"],"metadata":{"id":"zX-xwXzz9ZRH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649509008938,"user_tz":-480,"elapsed":67830,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"3ee7dd0a-4249-41ea-acc8-f5812b422d12"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: SentencePiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 30.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 40.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.3.15)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.5.1 sacremoses-0.0.49 tokenizers-0.11.6 transformers-4.18.0\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"lS3uicDQ9ZVG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UfbZ_pHj9ZXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT w preprocessing"],"metadata":{"id":"17U3J4Of9SMq"}},{"cell_type":"code","source":["## zx modification for bert\n","\n","def do_preprocessing(df_series):\n","  def punctuation_series(series):\n","    def punctuation(text):\n","      import re\n","      text = re.sub(r'[^a-zA-Z0-9?!\\.]', ' ', text)\n","      return text\n","    series = series.apply(punctuation)\n","    return series\n","  \n","  def emoji_series(series):\n","    import emoji\n","    series = series.apply(emoji.demojize) # Remove emojis\n","    return series\n","  \n","\n","     # add custom stopwords\n","    # default_stopwords = stopwords.DEFAULT\n","    # new_words = ['already','also','comment','delete','even','literally','lolol','lololol','lolz','lols','lot','loll','lolololol','lolll','mean','na',\n","    #             'point','post','probably','put','reddit','remove','see','something','want','well']\n","    # custom_stopwords = default_stopwords.union(set(new_words))\n","    # df_series = hero.preprocessing.remove_stopwords(df_series, custom_stopwords) # remove stopwords\n","    # lemmatize_text = df_series.apply(words_lemmatizer)\n","    \n","  custom_pipeline = [hero.preprocessing.fillna, #remove NA\n","                        hero.preprocessing.lowercase, # lowercase\n","                        hero.preprocessing.remove_urls, # remove urls\n","                        hero.preprocessing.remove_digits, # remove numbers\n","                        emoji_series, # deemoji\n","                        punctuation_series, #remove anything that doesnt help in Bert, only punctuation kept is .?!\n","                        hero.preprocessing.remove_whitespace # remove white spaces\n","                        ] \n","  df_series = df_series.pipe(hero.clean, custom_pipeline)\n","  return df_series"],"metadata":{"id":"WlY9a0OGx-yV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['clean'] = do_preprocessing(df['body'])"],"metadata":{"id":"mWSGT8Qryo6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['splitout']=df['clean'].apply([lambda word: word.split()])\n","df['len'] = df['splitout'].apply(lambda x:len(x))\n","\n","print('No. of data points with more than 100 words')\n","print(len(df[df['len']>100])/len(df))\n","print('No. of data points with more than 200 words')\n","print(len(df[df['len']>200])/len(df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73ww0S4Dyo9N","executionInfo":{"status":"ok","timestamp":1648878215313,"user_tz":-480,"elapsed":15,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"9fc552ac-c483-43e1-9dc2-498303f419de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["No. of data points with more than 100 words\n","0.012026239067055394\n","No. of data points with more than 200 words\n","0.0020043731778425656\n"]}]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n","def mask_inputs_for_bert(comments,max_len):\n","  input_ids = []\n","  attention_masks = []\n","  for comment in comments:\n","    encoded_dict = tokenizer.encode_plus(\n","        comment,\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        return_attention_mask= True\n","    )\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","  input_ids = tf.convert_to_tensor(input_ids)\n","  attention_masks = tf.convert_to_tensor(attention_masks)\n","  return input_ids, attention_masks\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["68f6512679b145929481c8411bac96db","5865bd39e32f43e78d78bd28bad09338","f52e232eeb2b437795fe80f3a09d51f4","63c5e7ce45a949088da12e6b2b9716ca","c36be66b06fa4c13b21cdffe4e93c8dd","5d057b61f296462c836948f7c10fbb34","ce892377d2e94886bc375962456f453a","733aa71e43294c2291f198a3718a1e2f","f7cae4e3609f4ff6bc30d8d82c062beb","281ef110af3a478ab10cc674e438dba5","f0ffe803085c4f6cad3bac7031f20d75","00946a24e060431698f283631b818811","1bf0a05be6ae4af0b1f00984ad05a67a","e6791faa2931456ba8168c6a71493eff","8ebcdae279f14b3da48e95dd0dd47ff4","f54129ae4bee4475aa7ac9b07b48aed1","052f0d52a79a45b884cf8d2ad1fb6fb3","84dae75de8554624b8125d5d8e5bc506","954ed14a16d841dfb5bcebceabdd8ccc","c8cd8aff7afc471ab8f83c09712b9ac9","ecbce6c5558a45f28dd2eb88faf810a2","74c427295f3e475a8980a4ebad8027e1","050f969d945f41f1b71a5d6ec4ccd568","ae9c8a067493416ca46cbd6ef32ab003","b99a8ba3d40a4c5aa5621d834739e9a4","26132a0445bd44abbf3405c45f7e14e6","dc374b94d938489d9a16463ea34ea57e","de016b5885b64d9ab062439498a21217","9dccd00cdfde4f918fafc4c99ea0b318","0850da5e960a4fd3bfa08c873ec35691","3de4f63bcbc04c118798458ea10cfe52","079c8999a121496b8152e875cfdc0261","d8309b7c01ab4910a6bb433a8284ef96"]},"id":"lGpdl9kYypIR","executionInfo":{"status":"ok","timestamp":1648878313594,"user_tz":-480,"elapsed":2511,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"1267d8ba-7c2d-4326-fe7e-8a8cf5d406cf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f6512679b145929481c8411bac96db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00946a24e060431698f283631b818811"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050f969d945f41f1b71a5d6ec4ccd568"}},"metadata":{}}]},{"cell_type":"code","source":["df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n","labels = df[\"sentiment\"].tolist()\n","labels_binary = df[\"sentiment_binary\"].tolist()\n","data = df['clean']\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data,labels_binary, test_size=0.25, random_state=42)\n","\n","max_len = 100\n","train_inp, train_mask  = mask_inputs_for_bert(X_train,max_len)\n","test_inp, test_mask = mask_inputs_for_bert(X_test,max_len)\n","train_label = tf.convert_to_tensor(y_train)\n","test_label = tf.convert_to_tensor(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9cl2NAryzZT","executionInfo":{"status":"ok","timestamp":1648883026899,"user_tz":-480,"elapsed":4456,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"04f469bb-3b53-490c-ba43-2bb44632c80a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n","def mask_inputs_for_bert(comments,max_len):\n","  input_ids = []\n","  attention_masks = []\n","  for comment in comments:\n","    encoded_dict = tokenizer.encode_plus(\n","        comment,\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        return_attention_mask= True\n","    )\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","  input_ids = tf.convert_to_tensor(input_ids)\n","  attention_masks = tf.convert_to_tensor(attention_masks)\n","  return input_ids, attention_masks\n","\n"],"metadata":{"id":"Yx7LTqAPBSaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n","labels_binary = df[\"sentiment_binary\"].tolist()\n","data = df['clean']\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data,labels_binary, test_size=0.25, random_state=42)\n","\n","max_len = 100\n","train_inp, train_mask  = mask_inputs_for_bert(X_train,max_len)\n","test_inp, test_mask = mask_inputs_for_bert(X_test,max_len)\n","train_label = tf.convert_to_tensor(y_train)\n","test_label = tf.convert_to_tensor(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648884523384,"user_tz":-480,"elapsed":9143,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"375b6df6-57b4-47c4-ee64-97f79529793a","id":"QHw8sMkwBSaK"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["def build_classifier_model():\n","  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  metrics = tf.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","  epochs = 30\n","  steps_per_epoch = 40 \n","  num_train_steps = steps_per_epoch * epochs\n","  num_warmup_steps = int(0.1*num_train_steps)\n","\n","  init_lr = 3e-5\n","  \n","  optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","  return model\n","with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n","  bert = build_classifier_model()\n","  \n","bert.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648884610716,"user_tz":-480,"elapsed":39825,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"2a2fa3fd-a3f3-437a-e9f8-fc3b8a20665e","id":"60-tH6GiBSaL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"tf_bert_for_sequence_classification_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  109482240 \n","                                                                 \n"," dropout_227 (Dropout)       multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 109,483,778\n","Trainable params: 109,483,778\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","mc = ModelCheckpoint('bert_project_v2.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only = True)\n","bert.fit([train_inp, train_mask],\\\n","                 train_label,\\\n","                 batch_size=100,\\\n","                 epochs = 30,\\\n","                 validation_data = ([test_inp, test_mask],test_label),\n","                callbacks = [mc])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648884981482,"user_tz":-480,"elapsed":365503,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"e2db52be-5fda-4d5c-cc11-4429a331482c","id":"ww3niVMIBSaM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","42/42 [==============================] - ETA: 0s - loss: 0.7099 - accuracy: 0.5078\n","Epoch 1: val_accuracy improved from -inf to 0.60714, saving model to bert_project_v2.h5\n","42/42 [==============================] - 174s 2s/step - loss: 0.7099 - accuracy: 0.5078 - val_loss: 0.6662 - val_accuracy: 0.6071\n","Epoch 2/30\n","42/42 [==============================] - ETA: 0s - loss: 0.6063 - accuracy: 0.6698\n","Epoch 2: val_accuracy improved from 0.60714 to 0.71574, saving model to bert_project_v2.h5\n","42/42 [==============================] - 9s 220ms/step - loss: 0.6063 - accuracy: 0.6698 - val_loss: 0.5490 - val_accuracy: 0.7157\n","Epoch 3/30\n","42/42 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7668\n","Epoch 3: val_accuracy improved from 0.71574 to 0.76239, saving model to bert_project_v2.h5\n","42/42 [==============================] - 9s 220ms/step - loss: 0.5062 - accuracy: 0.7668 - val_loss: 0.4965 - val_accuracy: 0.7624\n","Epoch 4/30\n","42/42 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8384\n","Epoch 4: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.3869 - accuracy: 0.8384 - val_loss: 0.5417 - val_accuracy: 0.7580\n","Epoch 5/30\n","42/42 [==============================] - ETA: 0s - loss: 0.2581 - accuracy: 0.9072\n","Epoch 5: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 7s 169ms/step - loss: 0.2581 - accuracy: 0.9072 - val_loss: 0.8904 - val_accuracy: 0.6764\n","Epoch 6/30\n","42/42 [==============================] - ETA: 0s - loss: 0.1889 - accuracy: 0.9373\n","Epoch 6: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 151ms/step - loss: 0.1889 - accuracy: 0.9373 - val_loss: 0.7511 - val_accuracy: 0.7398\n","Epoch 7/30\n","42/42 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9694\n","Epoch 7: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.1076 - accuracy: 0.9694 - val_loss: 0.8519 - val_accuracy: 0.7434\n","Epoch 8/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9801\n","Epoch 8: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0724 - accuracy: 0.9801 - val_loss: 0.9137 - val_accuracy: 0.7544\n","Epoch 9/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0477 - accuracy: 0.9866\n","Epoch 9: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0477 - accuracy: 0.9866 - val_loss: 1.0655 - val_accuracy: 0.7362\n","Epoch 10/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9920\n","Epoch 10: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0337 - accuracy: 0.9920 - val_loss: 1.0371 - val_accuracy: 0.7493\n","Epoch 11/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9927\n","Epoch 11: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0282 - accuracy: 0.9927 - val_loss: 1.1390 - val_accuracy: 0.7391\n","Epoch 12/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9947\n","Epoch 12: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0205 - accuracy: 0.9947 - val_loss: 1.2001 - val_accuracy: 0.7434\n","Epoch 13/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9961\n","Epoch 13: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0166 - accuracy: 0.9961 - val_loss: 1.2327 - val_accuracy: 0.7449\n","Epoch 14/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9966\n","Epoch 14: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 1.2958 - val_accuracy: 0.7383\n","Epoch 15/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9959\n","Epoch 15: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 7s 158ms/step - loss: 0.0130 - accuracy: 0.9959 - val_loss: 1.3469 - val_accuracy: 0.7427\n","Epoch 16/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9973\n","Epoch 16: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0109 - accuracy: 0.9973 - val_loss: 1.2900 - val_accuracy: 0.7529\n","Epoch 17/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9966\n","Epoch 17: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 1.4100 - val_accuracy: 0.7485\n","Epoch 18/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0092 - accuracy: 0.9973\n","Epoch 18: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 1.4566 - val_accuracy: 0.7493\n","Epoch 19/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9990\n","Epoch 19: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 1.4137 - val_accuracy: 0.7493\n","Epoch 20/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9983\n","Epoch 20: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 1.4367 - val_accuracy: 0.7515\n","Epoch 21/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9985\n","Epoch 21: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 1.4288 - val_accuracy: 0.7515\n","Epoch 22/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9985\n","Epoch 22: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 1.4544 - val_accuracy: 0.7566\n","Epoch 23/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9983\n","Epoch 23: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 1.4659 - val_accuracy: 0.7558\n","Epoch 24/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n","Epoch 24: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 1.4342 - val_accuracy: 0.7566\n","Epoch 25/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0030 - accuracy: 0.9990\n","Epoch 25: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 1.4511 - val_accuracy: 0.7515\n","Epoch 26/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0043 - accuracy: 0.9985\n","Epoch 26: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 1.4706 - val_accuracy: 0.7558\n","Epoch 27/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9993\n","Epoch 27: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 1.4814 - val_accuracy: 0.7566\n","Epoch 28/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9985\n","Epoch 28: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 6s 153ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 1.4768 - val_accuracy: 0.7551\n","Epoch 29/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0016 - accuracy: 0.9995\n","Epoch 29: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 7s 162ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 1.4816 - val_accuracy: 0.7558\n","Epoch 30/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0054 - accuracy: 0.9985\n","Epoch 30: val_accuracy did not improve from 0.76239\n","42/42 [==============================] - 7s 159ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 1.4816 - val_accuracy: 0.7558\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9482067490>"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["input, mask  = mask_inputs_for_bert(df['clean'],max_len)\n","target = df['sentiment_binary']\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import balanced_accuracy_score\n","print('Precision score = {}'.format(precision_score(target,prediction_class)))\n","print('Balanced Accuracy score = {}'.format(precision_score(target,prediction_class)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZA5ps9l8NK6k","executionInfo":{"status":"ok","timestamp":1648885067448,"user_tz":-480,"elapsed":32122,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"7ec3dbc2-916a-4f53-a110-65675e6a80aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Precision score = 0.9242310577644411\n","Balanced Accuracy score = 0.9242310577644411\n"]}]},{"cell_type":"code","source":["bert.load_weights('bert_project_v2.h5')\n","\n","input, mask  = mask_inputs_for_bert(df['clean'],max_len)\n","target = df['sentiment_binary']\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import balanced_accuracy_score\n","print('Precision score = {}'.format(precision_score(target,prediction_class)))\n","print('Balanced Accuracy score = {}'.format(precision_score(target,prediction_class)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648885362472,"user_tz":-480,"elapsed":35259,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"4eb64272-7278-4d59-85e8-eaf35d7ab62e","id":"prWUnyrfBSaN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Precision score = 0.8293424553401748\n","Balanced Accuracy score = 0.8293424553401748\n"]}]},{"cell_type":"code","source":["bert.load_weights('bert_project_v2.h5')\n","df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n","\n","input, mask  = mask_inputs_for_bert(df['clean'],max_len)\n","target = df['sentiment_binary']\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import balanced_accuracy_score\n","print('Precision score = {}'.format(precision_score(target,prediction_class)))\n","print('Balanced Accuracy score = {}'.format(precision_score(target,prediction_class)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"16eR6XXxPt6d","executionInfo":{"status":"ok","timestamp":1648888172881,"user_tz":-480,"elapsed":36178,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"6ae2b20f-3fb6-479e-dc42-136ca3c3d5a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Precision score = 0.8293424553401748\n","Balanced Accuracy score = 0.8293424553401748\n"]}]},{"cell_type":"code","source":["pd.Series(prediction_class).value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uZmgTa83OF4d","executionInfo":{"status":"ok","timestamp":1648885369081,"user_tz":-480,"elapsed":284,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"3047d1e2-2853-40a7-fdee-e31557b9a438"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2857\n","1    2631\n","dtype: int64"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":["frame = df.reset_index().drop('index',axis=1).join(pd.DataFrame(prediction[0],columns = ['Class_0 Logits BERT', 'CLass_1 Logits BERT']))\n","frame.to_csv('combined_tags_bert_logits_v1.csv')"],"metadata":{"id":"APMIO_dkG0pF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['clean'] = do_preprocessing(data['body'])"],"metadata":{"id":"axvp6S19G0sG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert.load_weights('bert_project_v2.h5')\n","\n","input, mask  = mask_inputs_for_bert(data['clean'],max_len)\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","pd.Series(prediction_class).value_counts(normalize=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mp4wwyPcG0un","executionInfo":{"status":"ok","timestamp":1648892038578,"user_tz":-480,"elapsed":1099408,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"cf8d3d5d-eaca-46dc-edf4-f72a8acd0fbe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["0    0.663597\n","1    0.336403\n","dtype: float64"]},"metadata":{},"execution_count":127}]},{"cell_type":"code","source":["frame = data.join(pd.DataFrame(prediction[0],columns = ['Class_0 Logits BERT', 'CLass_1 Logits BERT']))\n","frame.to_csv('combined_comments_bert_logits_v1.csv')"],"metadata":{"id":"DqVw_BAkG0xT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bert preprocessing with stop words removal "],"metadata":{"id":"aMPeynbn9lJ5"}},{"cell_type":"code","source":["## zx modification for bert\n","\n","def do_preprocessing_v2(df_series):\n","  def punctuation_series(series):\n","    def punctuation(text):\n","      import re\n","      text = re.sub(r'[^a-zA-Z0-9?!\\.]', ' ', text)\n","      return text\n","    series = series.apply(punctuation)\n","    return series\n","  \n","  def emoji_series(series):\n","    import emoji\n","    series = series.apply(emoji.demojize) # Remove emojis\n","    return series\n","  \n","\n","  #add custom stopwords\n","  # default_stopwords = stopwords.DEFAULT\n","\n","    \n","  custom_pipeline = [hero.preprocessing.fillna, #remove NA\n","                        hero.preprocessing.lowercase, # lowercase\n","                        hero.preprocessing.remove_urls, # remove urls\n","                        hero.preprocessing.remove_digits, # remove numbers\n","                        emoji_series, # deemoji\n","                        punctuation_series, #remove anything that doesnt help in Bert, only punctuation kept is .?!\n","                        hero.preprocessing.remove_whitespace # remove white spaces\n","                        ] \n","  df_series = df_series.pipe(hero.clean, custom_pipeline)\n","\n","  custom_stopwords = ['amc','bb','clov','pltr','gme','nok','meta','fb','amzn','wish','tsla']\n","  # custom_stopwords = default_stopwords.union(set(new_words))\n","  df_series = hero.preprocessing.remove_stopwords(df_series, custom_stopwords) # remove stopwords\n","  return df_series"],"metadata":{"id":"GmBHlcGV9O0H","executionInfo":{"status":"ok","timestamp":1649509017342,"user_tz":-480,"elapsed":350,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df['clean2'] = do_preprocessing_v2(df['body'])"],"metadata":{"id":"y7l1kp799PAF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n","def mask_inputs_for_bert(comments,max_len):\n","  input_ids = []\n","  attention_masks = []\n","  for comment in comments:\n","    encoded_dict = tokenizer.encode_plus(\n","        comment,\n","        add_special_tokens = True,\n","        max_length = max_len,\n","        pad_to_max_length = True,\n","        return_attention_mask= True\n","    )\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    attention_masks.append(encoded_dict['attention_mask'])\n","  input_ids = tf.convert_to_tensor(input_ids)\n","  attention_masks = tf.convert_to_tensor(attention_masks)\n","  return input_ids, attention_masks\n","\n"],"metadata":{"id":"9ac0SB5ERHAo","colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["523c3e24832c442b9424832503d2cbf2","66425f28dd474ac4a51caf270c6547b3","a66d04e7316d413ebf68e7377130145c","bb0cc5a6fe754e4c87a4210c6e24c71c","d0267f2000b94bd4aa887aee57bda5a9","8921a6559c60495a913f39d7e2fb2015","b4563b3104a940d19da9d96b3a4454fc","8d683c4a261f4bae83b44a9acd6496cb","66de9b01a867447ba968426805e34284","d16b556522804d409ae11f12f810f979","f3141fb7a1c1479b8c8e6c91bf21833a","b51eabc2bf7447beb824f7657664ec76","6d0815741116406e8ab2a6b0dbcb632d","3ac7b15b9e6b41dc9d2fb522617dedca","f0145c0b5e29456fbb7c7cb2e2844edd","cc038c69cd614cdbb388ff11828014e5","9a1e744e83334dc6bcb6d830f4c67dd8","3ad27e80bf69430b824b77ece0f5a089","f33be4f84ee241daa0d1ec21430dd50b","82ef32302e054c89891034546156b4b8","5c02930a5bcc4b8998b2248e3ffaabb1","3f89a46ece1d40c6a77c1fa10bac26a6","1d63d72b83654d5bae2d65d34bfbe969","a3a35df7721844068cf95305dd411563","4e98e7d1cdee4e3499269c4048f0389a","0aa33ed6bb3a4d06808337eca5ce449d","a0aa51c527b64cd79cec70b3755f0fd0","6bf4db606a014a4c9738b8ca7d288439","29a4a95ede4a44169390266920eb40f6","6f0f21815aca4363b365003375db1d1e","38db8014334b4ccdab4b7a0630e5d53a","1623bb2f0d324bf38863785b4a4e2d17","38c56d7c5edb4449ae519e1e5b131cd5"]},"executionInfo":{"status":"ok","timestamp":1649509022497,"user_tz":-480,"elapsed":2130,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"6bcb1d51-1c41-4b83-d235-152508f08d3f"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523c3e24832c442b9424832503d2cbf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b51eabc2bf7447beb824f7657664ec76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d63d72b83654d5bae2d65d34bfbe969"}},"metadata":{}}]},{"cell_type":"code","source":["df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n","labels_binary = df[\"sentiment_binary\"].tolist()\n","data = df['clean2']\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(data,labels_binary, test_size=0.25, random_state=42)\n","\n","max_len = 100\n","train_inp, train_mask  = mask_inputs_for_bert(X_train,max_len)\n","test_inp, test_mask = mask_inputs_for_bert(X_test,max_len)\n","train_label = tf.convert_to_tensor(y_train)\n","test_label = tf.convert_to_tensor(y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648886097286,"user_tz":-480,"elapsed":4405,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"6ba61108-624a-4db0-e6eb-c75ed304c615","id":"irCqZ8XLRHAo"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["def build_classifier_model():\n","  model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  metrics = tf.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","  epochs = 30\n","  steps_per_epoch = 40 \n","  num_train_steps = steps_per_epoch * epochs\n","  num_warmup_steps = int(0.1*num_train_steps)\n","\n","  init_lr = 3e-5\n","  \n","  optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                            num_train_steps=num_train_steps,\n","                                            num_warmup_steps=num_warmup_steps,\n","                                            optimizer_type='adamw')\n","  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","  return model\n","with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n","  bert = build_classifier_model()\n","  \n","bert.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399,"referenced_widgets":["b9335d79924a4373b309e6c121a1dafe","4cbf8f5aa5a4466eb48dfaa4180f6e3b","aabd1a98f1b240b392bd069d19d94141","c7122f22e7f94cff81b0f7d0e83ef664","cda5e57bb6864343802c3e1b0242c109","1983dc562c46466ba0bbdd74ac11e3ca","6abe69c752464604b84cc5d02e7012b6","7acb0dd179bf444d8fce34ef2fefac28","e361dfd6f68f42ce8405f1cb5234b90c","6b981d51756a4539af8167abafe43f92","331a52ca5b4f4f56bf46855ba92beb9b"]},"executionInfo":{"status":"ok","timestamp":1649509108752,"user_tz":-480,"elapsed":83011,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"e3cdede0-946f-41a3-9524-bfb93086c498","id":"7J1YgAkpRHAo"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9335d79924a4373b309e6c121a1dafe"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model: \"tf_bert_for_sequence_classification\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," bert (TFBertMainLayer)      multiple                  109482240 \n","                                                                 \n"," dropout_37 (Dropout)        multiple                  0         \n","                                                                 \n"," classifier (Dense)          multiple                  1538      \n","                                                                 \n","=================================================================\n","Total params: 109,483,778\n","Trainable params: 109,483,778\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n","mc = ModelCheckpoint('bert_project_v3.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only = True)\n","bert.fit([train_inp, train_mask],\\\n","                 train_label,\\\n","                 batch_size=100,\\\n","                 epochs = 30,\\\n","                 validation_data = ([test_inp, test_mask],test_label),\n","                callbacks = [mc])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648886934695,"user_tz":-480,"elapsed":366925,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"61c22a6e-4a02-496d-b1d0-efd08937e2d3","id":"tBkoqURrRHAp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","42/42 [==============================] - ETA: 0s - loss: 0.6983 - accuracy: 0.5323\n","Epoch 1: val_accuracy improved from -inf to 0.64431, saving model to bert_project_v3.h5\n","42/42 [==============================] - 167s 2s/step - loss: 0.6983 - accuracy: 0.5323 - val_loss: 0.6522 - val_accuracy: 0.6443\n","Epoch 2/30\n","42/42 [==============================] - ETA: 0s - loss: 0.6021 - accuracy: 0.6871\n","Epoch 2: val_accuracy improved from 0.64431 to 0.72012, saving model to bert_project_v3.h5\n","42/42 [==============================] - 9s 226ms/step - loss: 0.6021 - accuracy: 0.6871 - val_loss: 0.5640 - val_accuracy: 0.7201\n","Epoch 3/30\n","42/42 [==============================] - ETA: 0s - loss: 0.5064 - accuracy: 0.7619\n","Epoch 3: val_accuracy did not improve from 0.72012\n","42/42 [==============================] - 6s 148ms/step - loss: 0.5064 - accuracy: 0.7619 - val_loss: 0.5700 - val_accuracy: 0.7172\n","Epoch 4/30\n","42/42 [==============================] - ETA: 0s - loss: 0.3887 - accuracy: 0.8392\n","Epoch 4: val_accuracy improved from 0.72012 to 0.73324, saving model to bert_project_v3.h5\n","42/42 [==============================] - 12s 290ms/step - loss: 0.3887 - accuracy: 0.8392 - val_loss: 0.5776 - val_accuracy: 0.7332\n","Epoch 5/30\n","42/42 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9050\n","Epoch 5: val_accuracy improved from 0.73324 to 0.74781, saving model to bert_project_v3.h5\n","42/42 [==============================] - 9s 227ms/step - loss: 0.2655 - accuracy: 0.9050 - val_loss: 0.6914 - val_accuracy: 0.7478\n","Epoch 6/30\n","42/42 [==============================] - ETA: 0s - loss: 0.1803 - accuracy: 0.9410\n","Epoch 6: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.1803 - accuracy: 0.9410 - val_loss: 0.8020 - val_accuracy: 0.7420\n","Epoch 7/30\n","42/42 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9643\n","Epoch 7: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 7s 171ms/step - loss: 0.1194 - accuracy: 0.9643 - val_loss: 0.9068 - val_accuracy: 0.7281\n","Epoch 8/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9781\n","Epoch 8: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 151ms/step - loss: 0.0800 - accuracy: 0.9781 - val_loss: 1.1762 - val_accuracy: 0.7048\n","Epoch 9/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9847\n","Epoch 9: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0583 - accuracy: 0.9847 - val_loss: 1.0470 - val_accuracy: 0.7464\n","Epoch 10/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9917\n","Epoch 10: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0344 - accuracy: 0.9917 - val_loss: 1.2432 - val_accuracy: 0.7296\n","Epoch 11/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9925\n","Epoch 11: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0295 - accuracy: 0.9925 - val_loss: 1.2385 - val_accuracy: 0.7383\n","Epoch 12/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9944\n","Epoch 12: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0206 - accuracy: 0.9944 - val_loss: 1.3124 - val_accuracy: 0.7369\n","Epoch 13/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9954\n","Epoch 13: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 1.3620 - val_accuracy: 0.7354\n","Epoch 14/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9951\n","Epoch 14: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 1.4110 - val_accuracy: 0.7347\n","Epoch 15/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9966\n","Epoch 15: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 148ms/step - loss: 0.0134 - accuracy: 0.9966 - val_loss: 1.4090 - val_accuracy: 0.7434\n","Epoch 16/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9961\n","Epoch 16: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 8s 202ms/step - loss: 0.0134 - accuracy: 0.9961 - val_loss: 1.4409 - val_accuracy: 0.7347\n","Epoch 17/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0093 - accuracy: 0.9976\n","Epoch 17: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 1.4620 - val_accuracy: 0.7340\n","Epoch 18/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9985\n","Epoch 18: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 1.5041 - val_accuracy: 0.7340\n","Epoch 19/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9981\n","Epoch 19: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 149ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 1.5133 - val_accuracy: 0.7362\n","Epoch 20/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0071 - accuracy: 0.9978\n","Epoch 20: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 151ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 1.5625 - val_accuracy: 0.7362\n","Epoch 21/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0086 - accuracy: 0.9978\n","Epoch 21: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0086 - accuracy: 0.9978 - val_loss: 1.5599 - val_accuracy: 0.7376\n","Epoch 22/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0070 - accuracy: 0.9976\n","Epoch 22: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 150ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 1.5755 - val_accuracy: 0.7398\n","Epoch 23/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9993\n","Epoch 23: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 151ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 1.5924 - val_accuracy: 0.7376\n","Epoch 24/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n","Epoch 24: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 151ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 1.5983 - val_accuracy: 0.7391\n","Epoch 25/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0044 - accuracy: 0.9985\n","Epoch 25: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 152ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 1.6193 - val_accuracy: 0.7376\n","Epoch 26/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9990\n","Epoch 26: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 152ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 1.5981 - val_accuracy: 0.7391\n","Epoch 27/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0041 - accuracy: 0.9988\n","Epoch 27: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 152ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 1.6196 - val_accuracy: 0.7398\n","Epoch 28/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0060 - accuracy: 0.9988\n","Epoch 28: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 152ms/step - loss: 0.0060 - accuracy: 0.9988 - val_loss: 1.6056 - val_accuracy: 0.7398\n","Epoch 29/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9988\n","Epoch 29: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 7s 156ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 1.6096 - val_accuracy: 0.7420\n","Epoch 30/30\n","42/42 [==============================] - ETA: 0s - loss: 0.0027 - accuracy: 0.9988\n","Epoch 30: val_accuracy did not improve from 0.74781\n","42/42 [==============================] - 6s 151ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 1.6096 - val_accuracy: 0.7420\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9623916c90>"]},"metadata":{},"execution_count":87}]},{"cell_type":"code","source":["input, mask  = mask_inputs_for_bert(df['clean2'],max_len)\n","target = df['sentiment_binary']\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import balanced_accuracy_score\n","print('Precision score = {}'.format(precision_score(target,prediction_class)))\n","print('Balanced Accuracy score = {}'.format(precision_score(target,prediction_class)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648886957547,"user_tz":-480,"elapsed":22855,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"379f224b-7f07-4f09-efed-384509b78711","id":"Rqq0MSovRHAp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Precision score = 0.9298312883435583\n","Balanced Accuracy score = 0.9298312883435583\n"]}]},{"cell_type":"code","source":["bert.load_weights('bert_project_v3.h5')\n","\n","input, mask  = mask_inputs_for_bert(df['clean2'],max_len)\n","target = df['sentiment_binary']\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import balanced_accuracy_score\n","print('Precision score = {}'.format(precision_score(target,prediction_class)))\n","print('Balanced Accuracy score = {}'.format(precision_score(target,prediction_class)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648888867243,"user_tz":-480,"elapsed":35273,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"454848a8-b38f-4ed7-a280-573f1c86f8dc","id":"k5Q8Kk6jRHAq"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Precision score = 0.89223337049424\n","Balanced Accuracy score = 0.89223337049424\n"]}]},{"cell_type":"code","source":["pd.Series(prediction_class).value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1648888872766,"user_tz":-480,"elapsed":261,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"81878de0-8b24-4c48-9e1a-4d73b2699a98","id":"JKDOc8m3RHAq"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    2797\n","1    2691\n","dtype: int64"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["frame = df.reset_index().drop('index',axis=1).join(pd.DataFrame(prediction[0],columns = ['Class_0 Logits BERT', 'CLass_1 Logits BERT']))\n","frame.to_csv('combined_tags_bert_logits_v2.csv')"],"metadata":{"id":"gb7McHduRHAq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"alA56ghO9PCT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finally generating on full data"],"metadata":{"id":"lquELi98dKau"}},{"cell_type":"code","source":["data = pd.read_csv('combined_comments.csv')\n","data = data[data['body']!='[removed]']\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jcrYiynh9PFb","executionInfo":{"status":"ok","timestamp":1649509131595,"user_tz":-480,"elapsed":9430,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"cd062320-6389-4a24-c814-3b851188e8e1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (3,7) have mixed types.Specify dtype option on import or set low_memory=False.\n","  exec(code_obj, self.user_global_ns, self.user_ns)\n"]},{"output_type":"execute_result","data":{"text/plain":["               filename                author  \\\n","1        SUPRATIK_0.csv             okgeezeok   \n","2        SUPRATIK_0.csv  LegitimateInjury5720   \n","4        SUPRATIK_0.csv            jawnlerdoe   \n","7        SUPRATIK_0.csv          JokersKnight   \n","9        SUPRATIK_0.csv            JoeWelburg   \n","...                 ...                   ...   \n","1092975  ZONGXIAN_2.csv        MyNi_NotYourNi   \n","1092976  ZONGXIAN_2.csv        Ok_Bottle_2198   \n","1092977  ZONGXIAN_2.csv           inntw-inutw   \n","1092978  ZONGXIAN_2.csv         Forrest_GUHmp   \n","1092979  ZONGXIAN_2.csv             VisualMod   \n","\n","                                                      body   created_utc  \\\n","1                             https://youtu.be/gmq1ueWGKgY  1612139703.0   \n","2                                           And to da moon  1611968923.0   \n","4          I’m all in at open too. We’re in this together.  1611895308.0   \n","7             Invest it has such a strong really behind it  1611891585.0   \n","9              AMC will legit go through the roof tomorrow  1611889728.0   \n","...                                                    ...           ...   \n","1092975          ... and all they got was a weird haircut.    1639102296   \n","1092976  Musk fanboys are definitely the dumbest breed ...    1639102292   \n","1092977       Alright, ima head out .. to get my knee pads    1639102206   \n","1092978  Where's the loss porn? Did that guy with stupi...    1639101702   \n","1092979  \\n**User Report**| | | |\\n:--|:--|:--|:--\\n**T...    1639101536   \n","\n","              id    link_id  \\\n","1        gljgye5  t3_l7c2a3   \n","2        glbe5gt  t3_l7c2a3   \n","4        gl71p20  t3_l7c2a3   \n","7        gl6tpoi  t3_l7c2a3   \n","9        gl6pihh  t3_l7c2a3   \n","...          ...        ...   \n","1092975  hnxm4z1  t3_rcx695   \n","1092976  hnxm4nh  t3_rcx695   \n","1092977  hnxlxye  t3_rcx695   \n","1092978  hnxkt8u  t3_rcx695   \n","1092979  hnxkf9p  t3_rcx695   \n","\n","                                                 permalink score  \\\n","1        /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   1.0   \n","2        /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   1.0   \n","4        /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   1.0   \n","7        /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   1.0   \n","9        /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   2.0   \n","...                                                    ...   ...   \n","1092975  /r/wallstreetbets/comments/rcx695/129_tsla_spl...    27   \n","1092976  /r/wallstreetbets/comments/rcx695/129_tsla_spl...    26   \n","1092977  /r/wallstreetbets/comments/rcx695/129_tsla_spl...     7   \n","1092978  /r/wallstreetbets/comments/rcx695/129_tsla_spl...    14   \n","1092979  /r/wallstreetbets/comments/rcx695/129_tsla_spl...     1   \n","\n","              subreddit search_term  \\\n","1        wallstreetbets      l7c2a3   \n","2        wallstreetbets      l7c2a3   \n","4        wallstreetbets      l7c2a3   \n","7        wallstreetbets      l7c2a3   \n","9        wallstreetbets      l7c2a3   \n","...                 ...         ...   \n","1092975  wallstreetbets      rcx695   \n","1092976  wallstreetbets      rcx695   \n","1092977  wallstreetbets      rcx695   \n","1092978  wallstreetbets      rcx695   \n","1092979  wallstreetbets      rcx695   \n","\n","                                                 post_name  \n","1        Fuck the hedge funds; diamond hands. AMC to th...  \n","2        Fuck the hedge funds; diamond hands. AMC to th...  \n","4        Fuck the hedge funds; diamond hands. AMC to th...  \n","7        Fuck the hedge funds; diamond hands. AMC to th...  \n","9        Fuck the hedge funds; diamond hands. AMC to th...  \n","...                                                    ...  \n","1092975                          12/9 TSLA Split They Said  \n","1092976                          12/9 TSLA Split They Said  \n","1092977                          12/9 TSLA Split They Said  \n","1092978                          12/9 TSLA Split They Said  \n","1092979                          12/9 TSLA Split They Said  \n","\n","[810189 rows x 11 columns]"],"text/html":["\n","  <div id=\"df-1b7fedfc-bc25-4481-bb43-b748b4c06771\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>created_utc</th>\n","      <th>id</th>\n","      <th>link_id</th>\n","      <th>permalink</th>\n","      <th>score</th>\n","      <th>subreddit</th>\n","      <th>search_term</th>\n","      <th>post_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>okgeezeok</td>\n","      <td>https://youtu.be/gmq1ueWGKgY</td>\n","      <td>1612139703.0</td>\n","      <td>gljgye5</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>LegitimateInjury5720</td>\n","      <td>And to da moon</td>\n","      <td>1611968923.0</td>\n","      <td>glbe5gt</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>jawnlerdoe</td>\n","      <td>I’m all in at open too. We’re in this together.</td>\n","      <td>1611895308.0</td>\n","      <td>gl71p20</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>JokersKnight</td>\n","      <td>Invest it has such a strong really behind it</td>\n","      <td>1611891585.0</td>\n","      <td>gl6tpoi</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>JoeWelburg</td>\n","      <td>AMC will legit go through the roof tomorrow</td>\n","      <td>1611889728.0</td>\n","      <td>gl6pihh</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>2.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1092975</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>MyNi_NotYourNi</td>\n","      <td>... and all they got was a weird haircut.</td>\n","      <td>1639102296</td>\n","      <td>hnxm4z1</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>27</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","    </tr>\n","    <tr>\n","      <th>1092976</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>Ok_Bottle_2198</td>\n","      <td>Musk fanboys are definitely the dumbest breed ...</td>\n","      <td>1639102292</td>\n","      <td>hnxm4nh</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>26</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","    </tr>\n","    <tr>\n","      <th>1092977</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>inntw-inutw</td>\n","      <td>Alright, ima head out .. to get my knee pads</td>\n","      <td>1639102206</td>\n","      <td>hnxlxye</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>7</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","    </tr>\n","    <tr>\n","      <th>1092978</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>Forrest_GUHmp</td>\n","      <td>Where's the loss porn? Did that guy with stupi...</td>\n","      <td>1639101702</td>\n","      <td>hnxkt8u</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>14</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","    </tr>\n","    <tr>\n","      <th>1092979</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>VisualMod</td>\n","      <td>\\n**User Report**| | | |\\n:--|:--|:--|:--\\n**T...</td>\n","      <td>1639101536</td>\n","      <td>hnxkf9p</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>1</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>810189 rows × 11 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7fedfc-bc25-4481-bb43-b748b4c06771')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1b7fedfc-bc25-4481-bb43-b748b4c06771 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1b7fedfc-bc25-4481-bb43-b748b4c06771');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["data['clean2'] = do_preprocessing_v2(data['body'])"],"metadata":{"id":"QOCXz61hdPi1","executionInfo":{"status":"ok","timestamp":1649509195180,"user_tz":-480,"elapsed":56951,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["bert.load_weights('bert_project_v3.h5')\n","max_len=100\n","input, mask  = mask_inputs_for_bert(data['clean2'],max_len)\n","prediction = bert.predict([input, mask ])\n","prediction_class = prediction[0].argmax(axis=1)\n","pd.Series(prediction_class).value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CoTNJugdQDg","executionInfo":{"status":"ok","timestamp":1649510369164,"user_tz":-480,"elapsed":1121966,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"f3330034-7776-424c-9a08-c725c3d43197"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["0    499657\n","1    310532\n","dtype: int64"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["data.reset_index().drop('index',axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"qLgyVIMOew8G","executionInfo":{"status":"ok","timestamp":1649510446950,"user_tz":-480,"elapsed":1167,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"1c96df64-a2bd-4ea5-875c-fa54723a80c6"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              filename                author  \\\n","0       SUPRATIK_0.csv             okgeezeok   \n","1       SUPRATIK_0.csv  LegitimateInjury5720   \n","2       SUPRATIK_0.csv            jawnlerdoe   \n","3       SUPRATIK_0.csv          JokersKnight   \n","4       SUPRATIK_0.csv            JoeWelburg   \n","...                ...                   ...   \n","810184  ZONGXIAN_2.csv        MyNi_NotYourNi   \n","810185  ZONGXIAN_2.csv        Ok_Bottle_2198   \n","810186  ZONGXIAN_2.csv           inntw-inutw   \n","810187  ZONGXIAN_2.csv         Forrest_GUHmp   \n","810188  ZONGXIAN_2.csv             VisualMod   \n","\n","                                                     body   created_utc  \\\n","0                            https://youtu.be/gmq1ueWGKgY  1612139703.0   \n","1                                          And to da moon  1611968923.0   \n","2         I’m all in at open too. We’re in this together.  1611895308.0   \n","3            Invest it has such a strong really behind it  1611891585.0   \n","4             AMC will legit go through the roof tomorrow  1611889728.0   \n","...                                                   ...           ...   \n","810184          ... and all they got was a weird haircut.    1639102296   \n","810185  Musk fanboys are definitely the dumbest breed ...    1639102292   \n","810186       Alright, ima head out .. to get my knee pads    1639102206   \n","810187  Where's the loss porn? Did that guy with stupi...    1639101702   \n","810188  \\n**User Report**| | | |\\n:--|:--|:--|:--\\n**T...    1639101536   \n","\n","             id    link_id                                          permalink  \\\n","0       gljgye5  t3_l7c2a3  /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   \n","1       glbe5gt  t3_l7c2a3  /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   \n","2       gl71p20  t3_l7c2a3  /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   \n","3       gl6tpoi  t3_l7c2a3  /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   \n","4       gl6pihh  t3_l7c2a3  /r/wallstreetbets/comments/l7c2a3/fuck_the_hed...   \n","...         ...        ...                                                ...   \n","810184  hnxm4z1  t3_rcx695  /r/wallstreetbets/comments/rcx695/129_tsla_spl...   \n","810185  hnxm4nh  t3_rcx695  /r/wallstreetbets/comments/rcx695/129_tsla_spl...   \n","810186  hnxlxye  t3_rcx695  /r/wallstreetbets/comments/rcx695/129_tsla_spl...   \n","810187  hnxkt8u  t3_rcx695  /r/wallstreetbets/comments/rcx695/129_tsla_spl...   \n","810188  hnxkf9p  t3_rcx695  /r/wallstreetbets/comments/rcx695/129_tsla_spl...   \n","\n","       score       subreddit search_term  \\\n","0        1.0  wallstreetbets      l7c2a3   \n","1        1.0  wallstreetbets      l7c2a3   \n","2        1.0  wallstreetbets      l7c2a3   \n","3        1.0  wallstreetbets      l7c2a3   \n","4        2.0  wallstreetbets      l7c2a3   \n","...      ...             ...         ...   \n","810184    27  wallstreetbets      rcx695   \n","810185    26  wallstreetbets      rcx695   \n","810186     7  wallstreetbets      rcx695   \n","810187    14  wallstreetbets      rcx695   \n","810188     1  wallstreetbets      rcx695   \n","\n","                                                post_name  \\\n","0       Fuck the hedge funds; diamond hands. AMC to th...   \n","1       Fuck the hedge funds; diamond hands. AMC to th...   \n","2       Fuck the hedge funds; diamond hands. AMC to th...   \n","3       Fuck the hedge funds; diamond hands. AMC to th...   \n","4       Fuck the hedge funds; diamond hands. AMC to th...   \n","...                                                   ...   \n","810184                          12/9 TSLA Split They Said   \n","810185                          12/9 TSLA Split They Said   \n","810186                          12/9 TSLA Split They Said   \n","810187                          12/9 TSLA Split They Said   \n","810188                          12/9 TSLA Split They Said   \n","\n","                                                   clean2  \n","0                                                          \n","1                                          and to da moon  \n","2         i m all in at open too. we re in this together.  \n","3            invest it has such a strong really behind it  \n","4                 will legit go through the roof tomorrow  \n","...                                                   ...  \n","810184          ... and all they got was a weird haircut.  \n","810185  musk fanboys are definitely the dumbest breed ...  \n","810186        alright ima head out .. to get my knee pads  \n","810187  where s the loss porn? did that guy with stupi...  \n","810188  user report total submissions first seen in ws...  \n","\n","[810189 rows x 12 columns]"],"text/html":["\n","  <div id=\"df-41b966c0-3a25-437d-96d8-d4d235d2924d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filename</th>\n","      <th>author</th>\n","      <th>body</th>\n","      <th>created_utc</th>\n","      <th>id</th>\n","      <th>link_id</th>\n","      <th>permalink</th>\n","      <th>score</th>\n","      <th>subreddit</th>\n","      <th>search_term</th>\n","      <th>post_name</th>\n","      <th>clean2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>okgeezeok</td>\n","      <td>https://youtu.be/gmq1ueWGKgY</td>\n","      <td>1612139703.0</td>\n","      <td>gljgye5</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>LegitimateInjury5720</td>\n","      <td>And to da moon</td>\n","      <td>1611968923.0</td>\n","      <td>glbe5gt</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","      <td>and to da moon</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>jawnlerdoe</td>\n","      <td>I’m all in at open too. We’re in this together.</td>\n","      <td>1611895308.0</td>\n","      <td>gl71p20</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","      <td>i m all in at open too. we re in this together.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>JokersKnight</td>\n","      <td>Invest it has such a strong really behind it</td>\n","      <td>1611891585.0</td>\n","      <td>gl6tpoi</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>1.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","      <td>invest it has such a strong really behind it</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>SUPRATIK_0.csv</td>\n","      <td>JoeWelburg</td>\n","      <td>AMC will legit go through the roof tomorrow</td>\n","      <td>1611889728.0</td>\n","      <td>gl6pihh</td>\n","      <td>t3_l7c2a3</td>\n","      <td>/r/wallstreetbets/comments/l7c2a3/fuck_the_hed...</td>\n","      <td>2.0</td>\n","      <td>wallstreetbets</td>\n","      <td>l7c2a3</td>\n","      <td>Fuck the hedge funds; diamond hands. AMC to th...</td>\n","      <td>will legit go through the roof tomorrow</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>810184</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>MyNi_NotYourNi</td>\n","      <td>... and all they got was a weird haircut.</td>\n","      <td>1639102296</td>\n","      <td>hnxm4z1</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>27</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","      <td>... and all they got was a weird haircut.</td>\n","    </tr>\n","    <tr>\n","      <th>810185</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>Ok_Bottle_2198</td>\n","      <td>Musk fanboys are definitely the dumbest breed ...</td>\n","      <td>1639102292</td>\n","      <td>hnxm4nh</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>26</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","      <td>musk fanboys are definitely the dumbest breed ...</td>\n","    </tr>\n","    <tr>\n","      <th>810186</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>inntw-inutw</td>\n","      <td>Alright, ima head out .. to get my knee pads</td>\n","      <td>1639102206</td>\n","      <td>hnxlxye</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>7</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","      <td>alright ima head out .. to get my knee pads</td>\n","    </tr>\n","    <tr>\n","      <th>810187</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>Forrest_GUHmp</td>\n","      <td>Where's the loss porn? Did that guy with stupi...</td>\n","      <td>1639101702</td>\n","      <td>hnxkt8u</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>14</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","      <td>where s the loss porn? did that guy with stupi...</td>\n","    </tr>\n","    <tr>\n","      <th>810188</th>\n","      <td>ZONGXIAN_2.csv</td>\n","      <td>VisualMod</td>\n","      <td>\\n**User Report**| | | |\\n:--|:--|:--|:--\\n**T...</td>\n","      <td>1639101536</td>\n","      <td>hnxkf9p</td>\n","      <td>t3_rcx695</td>\n","      <td>/r/wallstreetbets/comments/rcx695/129_tsla_spl...</td>\n","      <td>1</td>\n","      <td>wallstreetbets</td>\n","      <td>rcx695</td>\n","      <td>12/9 TSLA Split They Said</td>\n","      <td>user report total submissions first seen in ws...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>810189 rows × 12 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41b966c0-3a25-437d-96d8-d4d235d2924d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-41b966c0-3a25-437d-96d8-d4d235d2924d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-41b966c0-3a25-437d-96d8-d4d235d2924d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["frame = data.reset_index().drop('index',axis=1).join(pd.DataFrame(prediction[0],columns = ['Class_0 Logits BERT', 'CLass_1 Logits BERT']))\n","frame.to_csv('combined_comments_bert_logits_v2.csv')"],"metadata":{"id":"J81trTWAdQFs","executionInfo":{"status":"ok","timestamp":1649510504429,"user_tz":-480,"elapsed":17396,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["df['sentiment_binary'].value_counts(normalize=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acy1ihrVdQIF","executionInfo":{"status":"ok","timestamp":1648890775074,"user_tz":-480,"elapsed":1169,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"a8e727e2-c2c7-4236-c2e0-6ea9660c977d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.526239\n","1    0.473761\n","Name: sentiment_binary, dtype: float64"]},"metadata":{},"execution_count":123}]},{"cell_type":"code","source":["pd.Series(prediction_class).value_counts(normalize=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H-P-YF3jIv1","executionInfo":{"status":"ok","timestamp":1648890788126,"user_tz":-480,"elapsed":3,"user":{"displayName":"Zong Xian Chua","userId":"17179767843072756322"}},"outputId":"079fea28-1f8b-4930-9e71-447df252ffd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    0.616717\n","1    0.383283\n","dtype: float64"]},"metadata":{},"execution_count":124}]},{"cell_type":"code","source":[""],"metadata":{"id":"X6ElooSLdQMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4y0IwAiadQOm"},"execution_count":null,"outputs":[]}]}