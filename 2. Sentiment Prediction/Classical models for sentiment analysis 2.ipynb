{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2189f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "import demoji\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d2c07a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>search_term</th>\n",
       "      <th>tagger</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_name</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentiment_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>glngetr</td>\n",
       "      <td>0</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>nevabraun</td>\n",
       "      <td>Thanks but youâ€™ve lost me at \\n\\nâ€žIf look at A...</td>\n",
       "      <td>1612214426</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>21</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>glndurd</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Awake_4E</td>\n",
       "      <td>Awesome ðŸ˜Ž! Why the moon ðŸ¤” Letâ€™s shoot AMC out ...</td>\n",
       "      <td>1612213448</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>25</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>glndfos</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Cloud9forreal</td>\n",
       "      <td>If you look at AMCs business page youâ€™ll find ...</td>\n",
       "      <td>1612213289</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>58</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>glnd58d</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>MacCoy69</td>\n",
       "      <td>I bought 20 today, i am also a retarded dumb m...</td>\n",
       "      <td>1612213186</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>13</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>glnd2xt</td>\n",
       "      <td>0</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Menuler</td>\n",
       "      <td>Ahh, my fellow Retard. I see the more and more...</td>\n",
       "      <td>1612213163</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>11</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       id  sentiment search_term    tagger         author  \\\n",
       "0      0  glngetr          0      ladvc3  SUPRATIK      nevabraun   \n",
       "1      1  glndurd          1      ladvc3  SUPRATIK       Awake_4E   \n",
       "2      2  glndfos          1      ladvc3  SUPRATIK  Cloud9forreal   \n",
       "3      3  glnd58d          1      ladvc3  SUPRATIK       MacCoy69   \n",
       "4      4  glnd2xt          0      ladvc3  SUPRATIK        Menuler   \n",
       "\n",
       "                                                body  created_utc    link_id  \\\n",
       "0  Thanks but youâ€™ve lost me at \\n\\nâ€žIf look at A...   1612214426  t3_ladvc3   \n",
       "1  Awesome ðŸ˜Ž! Why the moon ðŸ¤” Letâ€™s shoot AMC out ...   1612213448  t3_ladvc3   \n",
       "2  If you look at AMCs business page youâ€™ll find ...   1612213289  t3_ladvc3   \n",
       "3  I bought 20 today, i am also a retarded dumb m...   1612213186  t3_ladvc3   \n",
       "4  Ahh, my fellow Retard. I see the more and more...   1612213163  t3_ladvc3   \n",
       "\n",
       "                                           permalink  score       subreddit  \\\n",
       "0  /r/wallstreetbets/comments/ladvc3/just_bought_...     21  wallstreetbets   \n",
       "1  /r/wallstreetbets/comments/ladvc3/just_bought_...     25  wallstreetbets   \n",
       "2  /r/wallstreetbets/comments/ladvc3/just_bought_...     58  wallstreetbets   \n",
       "3  /r/wallstreetbets/comments/ladvc3/just_bought_...     13  wallstreetbets   \n",
       "4  /r/wallstreetbets/comments/ladvc3/just_bought_...     11  wallstreetbets   \n",
       "\n",
       "                                           post_name              filename  \\\n",
       "0  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "1  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "2  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "3  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "4  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "\n",
       "   sentiment_binary  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./comments_and_tags.csv', sep = ',', index_col = 'Unnamed: 0')\n",
    "# make the 3 class problem into a binary problem\n",
    "df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n",
    "df=df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ccc58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-e6e67da771fd>:18: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    pos= nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags -'JJ', 'JJR', 'JJS'\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags -'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags -'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags -'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "def remove_emoji(text):\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item,'')\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl= WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos= find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    new_words = ['already','also','comment','delete','even','literally','lolol','lololol','lolz','lols','lot','loll','lolololol','lolll','mean','na',\n",
    "                 'point','post','probably','put','reddit','remove','see','something','want','well'] #first 200 in dictionary_all #insert all additional stopwords you want to remove here \n",
    "    custom = nltk.corpus.stopwords.words('english')\n",
    "    custom.extend(new_words)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    removing_custom_words = [words for words in stopwords_removed if not words in custom]\n",
    "    return \" \".join(removing_custom_words)\n",
    "\n",
    "def do_prepocessing(one_row):\n",
    "    lower_text = one_row.lower()\n",
    "    \n",
    "    remove_url = re.sub(r'http\\S+', '',lower_text) # Remove URL\n",
    "    \n",
    "    remove_emoji_text = remove_emoji(remove_url) # Remove emojis\n",
    "\n",
    "    remove_unwanted_charectors = re.sub(r'[^a-zA-Z0-9_#@&\\s]', ' ', remove_emoji_text) # Remove unwanted charectors like punctuations andnon ascii \n",
    "    remove_unwanted_charectors = re.sub(r'&[\\w]+', ' ', remove_unwanted_charectors) # Remove &amp, *&words etc\n",
    "    \n",
    "    removed_extra_space = re.sub(r'\\s+',' ', remove_unwanted_charectors) # Remove extra white_spaces\n",
    "    \n",
    "    extract_hash = re.findall(r'#[\\w]+', removed_extra_space) # Extract #hashTags\n",
    "    extract_has_joined = \" \".join(extract_hash)\n",
    "    removed_hash_text = re.sub(r'#[\\w]+', '', removed_extra_space) # Remove #hastags\n",
    "    \n",
    "    remove_atrate = re.findall(r'@[\\w]+', removed_hash_text) # Extract @Users\n",
    "    removed_atrate = re.sub(r'@[\\w]+', '', removed_hash_text) # Remove @Users\n",
    "    \n",
    "    removed_stopwords_text = remove_stopwords(removed_atrate)\n",
    "    lemmatize_text = words_lemmatizer(removed_stopwords_text)\n",
    "    return lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "538ef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "df['body'] = df['body'].astype(str)\n",
    "df['body'] = df['body'].apply(do_prepocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af93584e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment_binary</th>\n",
       "      <th>zoppity</th>\n",
       "      <th>fomo day</th>\n",
       "      <th>fomo bought</th>\n",
       "      <th>fomo begin</th>\n",
       "      <th>fomo bag</th>\n",
       "      <th>fomo across</th>\n",
       "      <th>fomo absolute</th>\n",
       "      <th>follower subreddit</th>\n",
       "      <th>...</th>\n",
       "      <th>lie go</th>\n",
       "      <th>lie floor</th>\n",
       "      <th>lie fake</th>\n",
       "      <th>lie call</th>\n",
       "      <th>lick paint</th>\n",
       "      <th>lick butthole</th>\n",
       "      <th>lfg obligatory</th>\n",
       "      <th>liability legal</th>\n",
       "      <th>li nio</th>\n",
       "      <th>li</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks lose look amcs business page</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome moon let shoot amc solar system</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look amcs business page find actually business...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bought 20 today retard dumb money ape germany ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahh fellow retard people get pessimistic gme a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>hold 200 bb share hit moon</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>b ippity b oppity give zoppity</td>\n",
       "      <td>0</td>\n",
       "      <td>0.396874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>lol really force bb thing eh</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>remember bb stand billion billion</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5489 rows Ã— 7002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  sentiment_binary  \\\n",
       "0                   thanks lose look amcs business page                 0   \n",
       "1               awesome moon let shoot amc solar system                 1   \n",
       "2     look amcs business page find actually business...                 1   \n",
       "3     bought 20 today retard dumb money ape germany ...                 1   \n",
       "4     ahh fellow retard people get pessimistic gme a...                 0   \n",
       "...                                                 ...               ...   \n",
       "5484                         hold 200 bb share hit moon                 1   \n",
       "5485                     b ippity b oppity give zoppity                 0   \n",
       "5486                       lol really force bb thing eh                 0   \n",
       "5487                                                                    1   \n",
       "5488                  remember bb stand billion billion                 0   \n",
       "\n",
       "       zoppity  fomo day  fomo bought  fomo begin  fomo bag  fomo across  \\\n",
       "0     0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "1     0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "2     0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "3     0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "4     0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "...        ...       ...          ...         ...       ...          ...   \n",
       "5484  0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "5485  0.396874       0.0          0.0         0.0       0.0          0.0   \n",
       "5486  0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "5487  0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "5488  0.000000       0.0          0.0         0.0       0.0          0.0   \n",
       "\n",
       "      fomo absolute  follower subreddit  ...  lie go  lie floor  lie fake  \\\n",
       "0               0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "1               0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "2               0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "3               0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "4               0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "...             ...                 ...  ...     ...        ...       ...   \n",
       "5484            0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "5485            0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "5486            0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "5487            0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "5488            0.0                 0.0  ...     0.0        0.0       0.0   \n",
       "\n",
       "      lie call  lick paint  lick butthole  lfg obligatory  liability legal  \\\n",
       "0          0.0         0.0            0.0             0.0              0.0   \n",
       "1          0.0         0.0            0.0             0.0              0.0   \n",
       "2          0.0         0.0            0.0             0.0              0.0   \n",
       "3          0.0         0.0            0.0             0.0              0.0   \n",
       "4          0.0         0.0            0.0             0.0              0.0   \n",
       "...        ...         ...            ...             ...              ...   \n",
       "5484       0.0         0.0            0.0             0.0              0.0   \n",
       "5485       0.0         0.0            0.0             0.0              0.0   \n",
       "5486       0.0         0.0            0.0             0.0              0.0   \n",
       "5487       0.0         0.0            0.0             0.0              0.0   \n",
       "5488       0.0         0.0            0.0             0.0              0.0   \n",
       "\n",
       "      li nio   li  \n",
       "0        0.0  0.0  \n",
       "1        0.0  0.0  \n",
       "2        0.0  0.0  \n",
       "3        0.0  0.0  \n",
       "4        0.0  0.0  \n",
       "...      ...  ...  \n",
       "5484     0.0  0.0  \n",
       "5485     0.0  0.0  \n",
       "5486     0.0  0.0  \n",
       "5487     0.0  0.0  \n",
       "5488     0.0  0.0  \n",
       "\n",
       "[5489 rows x 7002 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "top_n = 7000\n",
    "\n",
    "#create new df with only body and sentiment\n",
    "dfNew = df[['body','sentiment_binary']]\n",
    "\n",
    "# create sparse matrix with 1-gram and 2-gram\n",
    "stop = set(stopwords.words('english'))\n",
    "corpus = dfNew.loc[:,'body']\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=list(stop))\n",
    "vectorized = tfidf.fit_transform(corpus)\n",
    "\n",
    "#convert to dense matrix\n",
    "vocab = tfidf.get_feature_names()\n",
    "df_vectorized= pd.DataFrame(vectorized.todense(),columns=vocab)\n",
    "\n",
    "#top k features\n",
    "indices = np.argsort(tfidf.idf_)[::-1]\n",
    "features = tfidf.get_feature_names()\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "\n",
    "#df with only top features\n",
    "df_top_vectorized = df_vectorized[top_features]\n",
    "\n",
    "# combine with dfNew\n",
    "df_combined = pd.concat([dfNew, df_top_vectorized], axis=1)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "949b6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined.drop(['body','sentiment_binary'],axis=1)\n",
    "y = df_combined['sentiment_binary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a48ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ffc2cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tohpi\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29/29 [12:27<00:00, 25.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "DummyClassifier                    0.50               0.50     0.50      0.50   \n",
      "AdaBoostClassifier                 0.53               0.50     0.50      0.37   \n",
      "LogisticRegression                 0.53               0.50     0.50      0.37   \n",
      "XGBClassifier                      0.53               0.50     0.50      0.37   \n",
      "SVC                                0.53               0.50     0.50      0.37   \n",
      "SGDClassifier                      0.53               0.50     0.50      0.37   \n",
      "RidgeClassifierCV                  0.53               0.50     0.50      0.37   \n",
      "RidgeClassifier                    0.53               0.50     0.50      0.37   \n",
      "RandomForestClassifier             0.53               0.50     0.50      0.37   \n",
      "QuadraticDiscriminantAnalysis      0.47               0.50     0.50      0.30   \n",
      "Perceptron                         0.47               0.50     0.50      0.30   \n",
      "PassiveAggressiveClassifier        0.53               0.50     0.50      0.37   \n",
      "NuSVC                              0.53               0.50     0.50      0.37   \n",
      "NearestCentroid                    0.53               0.50     0.50      0.37   \n",
      "LinearSVC                          0.53               0.50     0.50      0.37   \n",
      "BaggingClassifier                  0.53               0.50     0.50      0.37   \n",
      "LabelSpreading                     0.53               0.50     0.50      0.37   \n",
      "LabelPropagation                   0.53               0.50     0.50      0.37   \n",
      "KNeighborsClassifier               0.47               0.50     0.50      0.30   \n",
      "GaussianNB                         0.47               0.50     0.50      0.30   \n",
      "ExtraTreesClassifier               0.53               0.50     0.50      0.37   \n",
      "ExtraTreeClassifier                0.53               0.50     0.50      0.37   \n",
      "DecisionTreeClassifier             0.53               0.50     0.50      0.37   \n",
      "CategoricalNB                      0.53               0.50     0.50      0.37   \n",
      "CalibratedClassifierCV             0.53               0.50     0.50      0.37   \n",
      "LGBMClassifier                     0.53               0.50     0.50      0.37   \n",
      "BernoulliNB                        0.52               0.49     0.49      0.40   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "DummyClassifier                      1.16  \n",
      "AdaBoostClassifier                  11.91  \n",
      "LogisticRegression                   2.04  \n",
      "XGBClassifier                        6.31  \n",
      "SVC                                118.81  \n",
      "SGDClassifier                        2.41  \n",
      "RidgeClassifierCV                   30.85  \n",
      "RidgeClassifier                      3.75  \n",
      "RandomForestClassifier              25.59  \n",
      "QuadraticDiscriminantAnalysis       18.49  \n",
      "Perceptron                           1.67  \n",
      "PassiveAggressiveClassifier          2.61  \n",
      "NuSVC                              112.26  \n",
      "NearestCentroid                      1.39  \n",
      "LinearSVC                            3.94  \n",
      "BaggingClassifier                  158.56  \n",
      "LabelSpreading                       5.15  \n",
      "LabelPropagation                     5.03  \n",
      "KNeighborsClassifier                57.70  \n",
      "GaussianNB                           1.83  \n",
      "ExtraTreesClassifier                54.40  \n",
      "ExtraTreeClassifier                  1.87  \n",
      "DecisionTreeClassifier              50.03  \n",
      "CategoricalNB                        2.61  \n",
      "CalibratedClassifierCV              12.35  \n",
      "LGBMClassifier                       2.07  \n",
      "BernoulliNB                          1.63  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric = None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)\n",
    "# this is the score for top 7,000 terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42232704",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
