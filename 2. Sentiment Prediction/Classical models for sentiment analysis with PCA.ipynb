{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2189f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "import texthero as hero\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "import demoji\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d2c07a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>search_term</th>\n",
       "      <th>tagger</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_id</th>\n",
       "      <th>permalink</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_name</th>\n",
       "      <th>filename</th>\n",
       "      <th>sentiment_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>glngetr</td>\n",
       "      <td>0</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>nevabraun</td>\n",
       "      <td>Thanks but you‚Äôve lost me at \\n\\n‚ÄûIf look at A...</td>\n",
       "      <td>1612214426</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>21</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>glndurd</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Awake_4E</td>\n",
       "      <td>Awesome üòé! Why the moon ü§î Let‚Äôs shoot AMC out ...</td>\n",
       "      <td>1612213448</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>25</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>glndfos</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Cloud9forreal</td>\n",
       "      <td>If you look at AMCs business page you‚Äôll find ...</td>\n",
       "      <td>1612213289</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>58</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>glnd58d</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>MacCoy69</td>\n",
       "      <td>I bought 20 today, i am also a retarded dumb m...</td>\n",
       "      <td>1612213186</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>13</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>glnd2xt</td>\n",
       "      <td>1</td>\n",
       "      <td>ladvc3</td>\n",
       "      <td>SUPRATIK</td>\n",
       "      <td>Menuler</td>\n",
       "      <td>Ahh, my fellow Retard. I see the more and more...</td>\n",
       "      <td>1612213163</td>\n",
       "      <td>t3_ladvc3</td>\n",
       "      <td>/r/wallstreetbets/comments/ladvc3/just_bought_...</td>\n",
       "      <td>11</td>\n",
       "      <td>wallstreetbets</td>\n",
       "      <td>Just bought 860 of AMC, greetings from Germany...</td>\n",
       "      <td>comments_batch_6.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index       id  sentiment search_term    tagger         author  \\\n",
       "0      0  glngetr          0      ladvc3  SUPRATIK      nevabraun   \n",
       "1      1  glndurd          1      ladvc3  SUPRATIK       Awake_4E   \n",
       "2      2  glndfos          1      ladvc3  SUPRATIK  Cloud9forreal   \n",
       "3      3  glnd58d          1      ladvc3  SUPRATIK       MacCoy69   \n",
       "4      4  glnd2xt          1      ladvc3  SUPRATIK        Menuler   \n",
       "\n",
       "                                                body  created_utc    link_id  \\\n",
       "0  Thanks but you‚Äôve lost me at \\n\\n‚ÄûIf look at A...   1612214426  t3_ladvc3   \n",
       "1  Awesome üòé! Why the moon ü§î Let‚Äôs shoot AMC out ...   1612213448  t3_ladvc3   \n",
       "2  If you look at AMCs business page you‚Äôll find ...   1612213289  t3_ladvc3   \n",
       "3  I bought 20 today, i am also a retarded dumb m...   1612213186  t3_ladvc3   \n",
       "4  Ahh, my fellow Retard. I see the more and more...   1612213163  t3_ladvc3   \n",
       "\n",
       "                                           permalink  score       subreddit  \\\n",
       "0  /r/wallstreetbets/comments/ladvc3/just_bought_...     21  wallstreetbets   \n",
       "1  /r/wallstreetbets/comments/ladvc3/just_bought_...     25  wallstreetbets   \n",
       "2  /r/wallstreetbets/comments/ladvc3/just_bought_...     58  wallstreetbets   \n",
       "3  /r/wallstreetbets/comments/ladvc3/just_bought_...     13  wallstreetbets   \n",
       "4  /r/wallstreetbets/comments/ladvc3/just_bought_...     11  wallstreetbets   \n",
       "\n",
       "                                           post_name              filename  \\\n",
       "0  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "1  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "2  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "3  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "4  Just bought 860 of AMC, greetings from Germany...  comments_batch_6.csv   \n",
       "\n",
       "   sentiment_binary  \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\chuaz\\OneDrive\\MSBA\\Sem 4\\BT5153\\Project/comments_and_tags.csv', sep = ',', index_col = 'Unnamed: 0')\n",
    "# make the 3 class problem into a binary problem\n",
    "df['sentiment_binary'] = df['sentiment'].apply(lambda x: 1 if x == 1 else 0 )\n",
    "df=df.reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7008cce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\5153env\\lib\\site-packages\\ipykernel_launcher.py:18: FutureWarning:\n",
      "\n",
      "The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    pos= nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags -'JJ', 'JJR', 'JJS'\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags -'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags -'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags -'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "demoji.download_codes()\n",
    "\n",
    "def remove_emoji(text):\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item,'')\n",
    "        \n",
    "    return text\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl= WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos= find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    new_words = ['already','also','comment','delete','even','literally','lolol','lololol','lolz','lols','lot','loll','lolololol','lolll','mean','na',\n",
    "                 'point','post','probably','put','reddit','remove','see','something','want','well'] #first 200 in dictionary_all #insert all additional stopwords you want to remove here \n",
    "    custom = nltk.corpus.stopwords.words('english')\n",
    "    custom.extend(new_words)\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lang_stopwords = stopwords.words(lang)\n",
    "    stopwords_removed = [w for w in words if w.lower() not in lang_stopwords]\n",
    "    removing_custom_words = [words for words in stopwords_removed if not words in custom]\n",
    "    return \" \".join(removing_custom_words)\n",
    "\n",
    "def do_preprocessing(one_row):\n",
    "    lower_text = one_row.lower()\n",
    "    \n",
    "    remove_url = re.sub(r'http\\S+', '',lower_text) # Remove URL\n",
    "    \n",
    "    remove_emoji_text = remove_emoji(remove_url) # Remove emojis\n",
    "\n",
    "    remove_unwanted_charectors = re.sub(r'[^a-zA-Z0-9_#@&\\s]', ' ', remove_emoji_text) # Remove unwanted charectors like punctuations andnon ascii \n",
    "    remove_unwanted_charectors = re.sub(r'&[\\w]+', ' ', remove_unwanted_charectors) # Remove &amp, *&words etc\n",
    "    \n",
    "    removed_extra_space = re.sub(r'\\s+',' ', remove_unwanted_charectors) # Remove extra white_spaces\n",
    "    \n",
    "    extract_hash = re.findall(r'#[\\w]+', removed_extra_space) # Extract #hashTags\n",
    "    extract_has_joined = \" \".join(extract_hash)\n",
    "    removed_hash_text = re.sub(r'#[\\w]+', '', removed_extra_space) # Remove #hastags\n",
    "    \n",
    "    remove_atrate = re.findall(r'@[\\w]+', removed_hash_text) # Extract @Users\n",
    "    removed_atrate = re.sub(r'@[\\w]+', '', removed_hash_text) # Remove @Users\n",
    "    \n",
    "    removed_stopwords_text = remove_stopwords(removed_atrate)\n",
    "    lemmatize_text = words_lemmatizer(removed_stopwords_text)\n",
    "    return lemmatize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "538ef975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "df['body'] = df['body'].astype(str)\n",
    "df['body'] = df['body'].apply(do_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af93584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "top_n =250\n",
    "\n",
    "#create new df with only body and sentiment\n",
    "dfNew = df[['body','sentiment_binary']]\n",
    "\n",
    "# create sparse matrix with 1-gram and 2-gram\n",
    "stop = set(stopwords.words('english'))\n",
    "corpus = dfNew.loc[:,'body']\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words=list(stop), max_features= top_n)\n",
    "vectorized = tfidf.fit_transform(corpus)\n",
    "\n",
    "#convert to dense matrix\n",
    "vocab = tfidf.get_feature_names()\n",
    "df_vectorized= pd.DataFrame(vectorized.todense(),columns=vocab)\n",
    "\n",
    "# combine with dfNew\n",
    "df_combined = pd.concat([dfNew, df_vectorized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3773d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>sentiment_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thanks lose look amcs business page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome moon let shoot amc solar system</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>look amcs business page find actually business...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bought 20 today retard dumb money ape germany ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ahh fellow retard people get pessimistic gme a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>ape bb moon man wish bought share instead opti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>hold 200 bb share hit moon</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>b ippity b oppity give zoppity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>lol really force bb thing eh</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>remember bb stand billion billion</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5481 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  sentiment_binary\n",
       "0                   thanks lose look amcs business page                 0\n",
       "1               awesome moon let shoot amc solar system                 1\n",
       "2     look amcs business page find actually business...                 1\n",
       "3     bought 20 today retard dumb money ape germany ...                 1\n",
       "4     ahh fellow retard people get pessimistic gme a...                 1\n",
       "...                                                 ...               ...\n",
       "5482  ape bb moon man wish bought share instead opti...                 1\n",
       "5483                         hold 200 bb share hit moon                 1\n",
       "5484                     b ippity b oppity give zoppity                 0\n",
       "5485                       lol really force bb thing eh                 0\n",
       "5487                  remember bb stand billion billion                 0\n",
       "\n",
       "[5481 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "949b6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined.drop(['body','sentiment_binary'],axis=1)\n",
    "y = df_combined['sentiment_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8a4f6",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "931c4d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.10530335938127725\n",
      "10\n",
      "0.17065360678719577\n",
      "15\n",
      "0.2230978737889829\n",
      "20\n",
      "0.2680231063904331\n",
      "25\n",
      "0.3068484697578111\n",
      "30\n",
      "0.3429610095092213\n",
      "40\n",
      "0.40845648707643767\n",
      "50\n",
      "0.46614956029911925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "for i in [5,10,15,20,25,30,40,50]:\n",
    "    pca = PCA(n_components=i)\n",
    "    principalComponents = pca.fit_transform(df_vectorized)\n",
    "    print(i)\n",
    "    print(pca.explained_variance_ratio_.cumsum()[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87783065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5488"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a48ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=50)\n",
    "principalComponents = pca.fit_transform(df_vectorized)\n",
    "X_train, X_test, y_train, y_test = train_test_split(principalComponents, y, test_size=0.3, random_state=1, stratify =y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3377c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chuaz\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning:\n",
      "\n",
      "The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "\n",
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                       | 4/29 [00:04<00:28,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CategoricalNB model failed to execute\n",
      "Negative values in data passed to CategoricalNB (input X)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå        | 26/29 [00:14<00:01,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StackingClassifier model failed to execute\n",
      "__init__() missing 1 required positional argument: 'estimators'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:16<00:00,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LGBMClassifier                     0.66               0.66     0.66      0.66   \n",
      "ExtraTreesClassifier               0.66               0.65     0.65      0.66   \n",
      "RandomForestClassifier             0.65               0.65     0.65      0.65   \n",
      "SVC                                0.65               0.65     0.65      0.65   \n",
      "CalibratedClassifierCV             0.65               0.64     0.64      0.65   \n",
      "LogisticRegression                 0.65               0.64     0.64      0.65   \n",
      "LinearSVC                          0.65               0.64     0.64      0.65   \n",
      "RidgeClassifierCV                  0.65               0.64     0.64      0.64   \n",
      "LinearDiscriminantAnalysis         0.65               0.64     0.64      0.64   \n",
      "RidgeClassifier                    0.65               0.64     0.64      0.64   \n",
      "NearestCentroid                    0.64               0.64     0.64      0.64   \n",
      "XGBClassifier                      0.64               0.64     0.64      0.64   \n",
      "QuadraticDiscriminantAnalysis      0.64               0.63     0.63      0.64   \n",
      "NuSVC                              0.64               0.63     0.63      0.63   \n",
      "AdaBoostClassifier                 0.63               0.63     0.63      0.63   \n",
      "BernoulliNB                        0.63               0.63     0.63      0.63   \n",
      "SGDClassifier                      0.63               0.62     0.62      0.62   \n",
      "PassiveAggressiveClassifier        0.62               0.62     0.62      0.62   \n",
      "BaggingClassifier                  0.61               0.61     0.61      0.61   \n",
      "GaussianNB                         0.60               0.60     0.60      0.60   \n",
      "Perceptron                         0.59               0.59     0.59      0.59   \n",
      "DecisionTreeClassifier             0.59               0.58     0.58      0.59   \n",
      "KNeighborsClassifier               0.58               0.58     0.58      0.58   \n",
      "LabelSpreading                     0.58               0.58     0.58      0.58   \n",
      "LabelPropagation                   0.58               0.58     0.58      0.58   \n",
      "ExtraTreeClassifier                0.57               0.57     0.57      0.57   \n",
      "DummyClassifier                    0.48               0.48     0.48      0.48   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LGBMClassifier                       0.56  \n",
      "ExtraTreesClassifier                 0.76  \n",
      "RandomForestClassifier               1.85  \n",
      "SVC                                  1.30  \n",
      "CalibratedClassifierCV               2.16  \n",
      "LogisticRegression                   0.04  \n",
      "LinearSVC                            0.59  \n",
      "RidgeClassifierCV                    0.05  \n",
      "LinearDiscriminantAnalysis           0.06  \n",
      "RidgeClassifier                      0.05  \n",
      "NearestCentroid                      0.02  \n",
      "XGBClassifier                        0.94  \n",
      "QuadraticDiscriminantAnalysis        0.07  \n",
      "NuSVC                                1.50  \n",
      "AdaBoostClassifier                   1.11  \n",
      "BernoulliNB                          0.03  \n",
      "SGDClassifier                        0.08  \n",
      "PassiveAggressiveClassifier          0.05  \n",
      "BaggingClassifier                    1.21  \n",
      "GaussianNB                           0.03  \n",
      "Perceptron                           0.03  \n",
      "DecisionTreeClassifier               0.23  \n",
      "KNeighborsClassifier                 0.72  \n",
      "LabelSpreading                       1.42  \n",
      "LabelPropagation                     1.17  \n",
      "ExtraTreeClassifier                  0.03  \n",
      "DummyClassifier                      0.03  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=False, predictions = True, custom_metric = None, random_state = 42 )\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "print(models)\n",
    "# this is the score for top 250 terms + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "560f9525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6363636363636364"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:5153env]",
   "language": "python",
   "name": "conda-env-5153env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
